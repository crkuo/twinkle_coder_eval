#!/usr/bin/env python3
"""
å®Œæ•´æµç¨‹æ¸¬è©¦ - æ¨¡æ“¬æ•´å€‹ training-evaluation pipeline
åŒ…å«é…ç½®è§£æã€æ¨¡å‹å‰µå»ºã€åŸºæº–æ¸¬è©¦ã€è©•ä¼°æµç¨‹ç­‰
"""
import os
import sys
import yaml
import json
from pathlib import Path

# æ·»åŠ è·¯å¾‘
sys.path.extend(['.', '..'])

def test_configuration_system():
    """æ¸¬è©¦é…ç½®ç³»çµ±"""
    print("ğŸ”§ æ¸¬è©¦é…ç½®ç³»çµ±...")
    
    # æ¸¬è©¦ YAML é…ç½®è¼‰å…¥
    with open('configs/test_mbpp_config.yml', 'r') as f:
        config = yaml.safe_load(f)
    
    print(f"   - é…ç½®åç¨±: {config['name']}")
    print(f"   - æ¨¡å‹å¾Œç«¯: {config['model']['backend'][0]['type']}")
    print(f"   - åŸºæº–æ¸¬è©¦: {config['evaluation']['benchmark'][0]['type']}")
    
    # æ¸¬è©¦é…ç½®é©—è­‰
    required_keys = ['name', 'model', 'evaluation']
    for key in required_keys:
        assert key in config, f"Missing required key: {key}"
    
    print("âœ… é…ç½®ç³»çµ±æ­£å¸¸")
    return config

def test_benchmark_system():
    """æ¸¬è©¦åŸºæº–æ¸¬è©¦ç³»çµ±"""
    print("ğŸ¯ æ¸¬è©¦åŸºæº–æ¸¬è©¦ç³»çµ±...")
    
    from benchmark.MBPP.MBPP import MBPP
    from factory import BenchmarkFactory
    
    # ç›´æ¥å‰µå»º MBPP å¯¦ä¾‹
    mbpp = MBPP(name="MBPP", prompt_type="Instruction")
    print(f"   - å‰µå»ºåŸºæº–æ¸¬è©¦: {mbpp.name}")
    print(f"   - æç¤ºé¡å‹: {mbpp.prompt_type}")
    
    # æ¸¬è©¦å·¥å» å‰µå»º
    class MockArgs:
        def __init__(self):
            self.task = "MBPP"
            self.prompt_type = "Instruction"
    
    args = MockArgs()
    task = BenchmarkFactory.get_task(args)
    print(f"   - å·¥å» å‰µå»º: {task.name}")
    
    # æ¸¬è©¦åŸºæœ¬æ–¹æ³•å­˜åœ¨
    methods = ['get_prompt', 'postprocess_generation', 'process_results', 'prepare_dataset']
    for method in methods:
        assert hasattr(task, method), f"Missing method: {method}"
    
    print("âœ… åŸºæº–æ¸¬è©¦ç³»çµ±æ­£å¸¸")
    return task

def test_backend_interface():
    """æ¸¬è©¦å¾Œç«¯æ¥å£ï¼ˆä¸å•Ÿå‹•å¯¦éš›æ¨¡å‹ï¼‰"""
    print("ğŸ¤– æ¸¬è©¦å¾Œç«¯æ¥å£...")
    
    from backend.base import Generator
    
    # æ¸¬è©¦åŸºç¤æŠ½è±¡é¡
    assert hasattr(Generator, 'generate'), "Missing generate method"
    
    # æ¸¬è©¦ VLLM å¾Œç«¯é¡çµæ§‹ï¼ˆä¸åˆå§‹åŒ–ï¼‰
    try:
        from backend.vllm.vllm import VllmGenerator
        
        # æª¢æŸ¥é¡å®šç¾©
        assert hasattr(VllmGenerator, '__init__'), "Missing __init__ method"
        assert hasattr(VllmGenerator, 'generate'), "Missing generate method"
        
        print("   - VLLM å¾Œç«¯æ¥å£çµæ§‹æ­£å¸¸")
        
    except ImportError as e:
        print(f"   - âš ï¸ VLLM å¾Œç«¯å°å…¥å•é¡Œ: {e}")
    
    print("âœ… å¾Œç«¯æ¥å£æ­£å¸¸")

def test_evaluation_pipeline():
    """æ¸¬è©¦è©•ä¼°æµç¨‹ï¼ˆæ¨¡æ“¬æ•¸æ“šï¼‰"""
    print("ğŸ“Š æ¸¬è©¦è©•ä¼°æµç¨‹...")
    
    from utils import refine_text, write_jsonl, group_and_count, estimate_pass_at_k
    
    # æ¨¡æ“¬è©•ä¼°æ•¸æ“š
    mock_generations = [
        {'task_id': 'test_1', 'generation': 'def solution():\n    return True'},
        {'task_id': 'test_2', 'generation': 'def solution():\n    return False'},
    ]
    
    # æ¨¡æ“¬è©•ä¼°çµæœ
    mock_evaluations = [
        {'task_id': 'test_1', 'passed': True},
        {'task_id': 'test_2', 'passed': False},
    ]
    
    # æ¸¬è©¦çµæœåˆ†çµ„å’Œè¨ˆç®—
    result_list = group_and_count(mock_evaluations, group_key='task_id', count_key='passed')
    assert result_list == [1, 0], f"Expected [1, 0], got {result_list}"
    
    # æ¸¬è©¦ Pass@K è¨ˆç®—
    pass_rate = estimate_pass_at_k(num_samples=1, num_correct=result_list, k=1)
    expected_rate = 0.5  # 1 passed out of 2
    
    print(f"   - æ¨¡æ“¬ Pass@1: {pass_rate[0]:.2f}")
    print(f"   - é æœŸå€¼: {expected_rate}")
    
    print("âœ… è©•ä¼°æµç¨‹æ­£å¸¸")

def test_unified_workflow():
    """æ¸¬è©¦çµ±ä¸€å·¥ä½œæµç¨‹"""
    print("ğŸ”„ æ¸¬è©¦çµ±ä¸€å·¥ä½œæµç¨‹...")
    
    # å‰µå»ºæ¸¬è©¦è¼¸å‡ºç›®éŒ„
    test_output_dir = Path("./test_workflow_output")
    test_output_dir.mkdir(exist_ok=True)
    
    # æ­¥é©Ÿ 1: é…ç½®è§£æ
    config = test_configuration_system()
    
    # æ­¥é©Ÿ 2: åƒæ•¸å‰µå»º
    class WorkflowArgs:
        def __init__(self, config):
            model_config = config.get('model', {})
            backend_config = model_config.get('backend', [{}])[0]
            eval_config = config.get('evaluation', {})
            benchmark_config = eval_config.get('benchmark', [{}])[0]
            
            self.model_name = backend_config.get('model_name', 'test_model')
            self.task = benchmark_config.get('type', 'MBPP')
            self.prompt_type = benchmark_config.get('prompt_type', 'Instruction')
            self.num_samples = eval_config.get('num_samples', 2)
            self.save_path = str(test_output_dir)
    
    args = WorkflowArgs(config)
    print(f"   - å·¥ä½œæµç¨‹åƒæ•¸: {args.task}, {args.model_name}")
    
    # æ­¥é©Ÿ 3: åŸºæº–æ¸¬è©¦å‰µå»º
    from factory import BenchmarkFactory
    task = BenchmarkFactory.get_task(args)
    print(f"   - åŸºæº–æ¸¬è©¦è¼‰å…¥: {task.name}")
    
    # æ­¥é©Ÿ 4: æ¨¡æ“¬è©•ä¼°æµç¨‹
    print("   - æ¨¡æ“¬ç”Ÿæˆå’Œè©•ä¼°...")
    
    # æ¨¡æ“¬æç¤ºç”Ÿæˆ
    mock_prompts = [
        {'task_id': 'mock_1', 'prompt': 'Write a function to add two numbers'},
        {'task_id': 'mock_2', 'prompt': 'Write a function to multiply two numbers'}
    ]
    
    # æ¨¡æ“¬ç”Ÿæˆçµæœ
    mock_generations = [
        {'task_id': 'mock_1', 'generation': 'def add(a, b):\n    return a + b'},
        {'task_id': 'mock_2', 'generation': 'def multiply(a, b):\n    return a * b'}
    ]
    
    # æ¨¡æ“¬è©•ä¼°çµæœ
    mock_evaluations = [
        {'task_id': 'mock_1', 'passed': True},
        {'task_id': 'mock_2', 'passed': True}
    ]
    
    # ä¿å­˜çµæœ
    from utils import write_jsonl
    write_jsonl(test_output_dir / "mock_prompts.jsonl", mock_prompts)
    write_jsonl(test_output_dir / "mock_generations.jsonl", mock_generations)
    write_jsonl(test_output_dir / "mock_evaluations.jsonl", mock_evaluations)
    
    # è¨ˆç®—æœ€çµ‚åˆ†æ•¸
    from utils import group_and_count, estimate_pass_at_k
    import numpy as np
    
    result_list = group_and_count(mock_evaluations, group_key='task_id', count_key='passed')
    pass_rate = float(np.mean(estimate_pass_at_k(num_samples=1, num_correct=result_list, k=1)))
    
    final_result = {"score": pass_rate, "pass_at_1": pass_rate}
    write_jsonl(test_output_dir / "final_result.json", [final_result])
    
    print(f"   - æœ€çµ‚ Pass@1: {pass_rate:.2f}")
    print(f"   - çµæœä¿å­˜è‡³: {test_output_dir}")
    
    print("âœ… çµ±ä¸€å·¥ä½œæµç¨‹æ­£å¸¸")
    
    return pass_rate

def test_mmocr_style_integration():
    """æ¸¬è©¦ MMOCR é¢¨æ ¼çš„é›†æˆ"""
    print("ğŸ—ï¸ æ¸¬è©¦ MMOCR é¢¨æ ¼é›†æˆ...")
    
    # æ¸¬è©¦ metafile è¼‰å…¥
    metafile_path = "benchmark/MBPP/metafile.yml"
    if os.path.exists(metafile_path):
        with open(metafile_path, 'r') as f:
            metafile = yaml.safe_load(f)
        
        print(f"   - Metafile è¼‰å…¥: {metafile['Name']}")
        print(f"   - è«–æ–‡æ¨™é¡Œ: {metafile['Paper']['Title'][:50]}...")
        print(f"   - æ•¸æ“šæ ¼å¼: {metafile['Data']['Format']}")
    
    # æ¸¬è©¦è¨»å†Šç³»çµ±åŸºæœ¬çµæ§‹
    try:
        from core.instance_manager import InstanceManager
        print("   - å¯¦ä¾‹ç®¡ç†å™¨å¯ç”¨")
    except ImportError:
        print("   - âš ï¸ å¯¦ä¾‹ç®¡ç†å™¨æœªå¯¦ç¾")
    
    # æ¸¬è©¦å·¥å» æ¨¡å¼
    from factory import BenchmarkFactory
    print("   - å·¥å» æ¨¡å¼æ­£å¸¸é‹è¡Œ")
    
    print("âœ… MMOCR é¢¨æ ¼é›†æˆæ­£å¸¸")

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹å®Œæ•´æµç¨‹æ¸¬è©¦\n")
    
    # åˆ‡æ›åˆ°æ­£ç¢ºç›®éŒ„
    os.chdir('/home/edward/twinkle_code_eval/refactor')
    
    try:
        # æ¸¬è©¦å„å€‹çµ„ä»¶
        print("=" * 50)
        test_configuration_system()
        
        print("\n" + "=" * 50)
        test_benchmark_system()
        
        print("\n" + "=" * 50)
        test_backend_interface()
        
        print("\n" + "=" * 50)
        test_evaluation_pipeline()
        
        print("\n" + "=" * 50)
        final_score = test_unified_workflow()
        
        print("\n" + "=" * 50)
        test_mmocr_style_integration()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ å®Œæ•´æµç¨‹æ¸¬è©¦å®Œæˆï¼")
        print("\nğŸ“‹ æ¸¬è©¦ç¸½çµ:")
        print("   âœ… é…ç½®ç³»çµ± - YAML é…ç½®è§£ææ­£å¸¸")
        print("   âœ… åŸºæº–æ¸¬è©¦ç³»çµ± - MBPP åŸºæº–æ¸¬è©¦è¼‰å…¥å’Œå·¥å» å‰µå»ºæ­£å¸¸")
        print("   âœ… å¾Œç«¯æ¥å£ - æ¨¡å‹å¾Œç«¯æ¥å£çµæ§‹æ­£å¸¸")
        print("   âœ… è©•ä¼°æµç¨‹ - Pass@K è¨ˆç®—å’Œçµæœè™•ç†æ­£å¸¸")
        print("   âœ… çµ±ä¸€å·¥ä½œæµç¨‹ - ç«¯åˆ°ç«¯æµç¨‹æ¨¡æ“¬æˆåŠŸ")
        print("   âœ… MMOCR é¢¨æ ¼é›†æˆ - é…ç½®å’Œå·¥å» æ¨¡å¼æ­£å¸¸")
        
        print(f"\nğŸ¯ æ¨¡æ“¬è©•ä¼°åˆ†æ•¸: {final_score:.2f}")
        print("\nâœ¨ é‡æ§‹æ¡†æ¶å·²æˆåŠŸæ•´åˆ OpenCoder å’Œ MMOCR çš„æ ¸å¿ƒåŠŸèƒ½ï¼")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)